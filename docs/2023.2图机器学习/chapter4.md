# Chapter04 图嵌入表示
## 4.1 图嵌入概述(Node Embeddings)
机器学习任务拥有好的特征，就好比做饭拥有好的食材。</br>
图机器学习同样，将图的信息转换为计算机能处理的信息，是进行后续训练的基础。
- 传统图机器学习：通过人工选取特征，将信息转换为向量。
- 图的表示学习：自动学习特征，将各种模态的输入转为向量。
### 4.1.1 图嵌入
- 目标：将图的信息没有丢失的翻译为机器能够处理的信息，能够与下游任务相互独立。
- 方法：将节点映射为 $d$ 维向量，节点之间的连接对应于矩阵变换，从而将图嵌入到 $R^d$ 空间中。
- 特点：</br>
1.低维：向量维度远小于节点数</br>
2.连续：每个元素都是实数</br>
3.稠密：每个元素都不为0

## 4.2 图嵌入的基本框架：编码器-解码器
假定图 $G$ ，$\mathcal{V}$ 是节点集，$A$ 是邻接矩阵
### 4.2.1 编码器
- Encoder(ENC)：将节点 $v{\in}\mathcal{V}$ 映射到vector embeddings $z_v{\in}\mathbb{R}^d$：
$${\rm{ENC}}(v)=z_v$$
- 最简单版本的编码器(浅编码器)：</br>
采用查表的思路，用 $Z$ 矩阵来记录节点嵌入，用one-hot向量 $v$ 来提取嵌入向量：
$${\rm{ENC}}(v)=x_v=Z{\cdot}v$$
- 编码器优化方案：DeepWalk、Node2Vec
### 4.2.2 解码器
- Decoder(DEC)：解码器用来预测两个节点的关系或相似度，具备以下形式：
$${\rm{DEC}}:\ \mathbb{R}^d\times\mathbb{R}^d\rightarrow\mathbb{R}^+$$
- 简单版本的解码器：</br>
以向量点乘结果来对应节点相似度：
$${\rm{similarity}}(u,v)\ {\approx}\ {z_v}^{\top}z_u$$
- 节点相似度定义方式(根据具体问题人工定义)：</br>
1.两节点直接相连则节点更相似</br>
2.两节点间接相连(有共同邻点或邻点集)则节点更相似</br>
3.有相同的功能角色则节点更相似</br>
4....
- 解码器优化方案：用随机游走来代替向量点乘求节点相似度。
### 4.2.3 编码器-解码器模型优化迭代
- 在训练集 $\mathcal{D}$ 上迭代优化，目的是最小化empirical reconstruction loss：
$$\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}\ell({\rm{DEC}}(z_u,z_v),{\rm{similarity}}(u,v))$$
其中 $\ell$ 是损失函数，用来衡量解码器结果和人工定义的similarity的差异，可以是均方误差或者是crossentropyloss等。

## 4.3 基于随机游走的方法
### 4.3.1 随机游走算法概述
- 从一个或一组节点出发，遍历整张图。在任意节点以一定概率随机游走到下一节点。$T$ 步后得到一个随机游走序列，迭代多次后得到一个收敛的概率分布，用来计算节点相似度。

### 4.3.2 节点相似度
- 定义两个节点相似为：共同出现在同一个随机游走序列。
- 从节点 $u$ 进行长度为 $T$ 的随机游走，最终到达 $v$ 点的概率定义为$p_{\mathcal{G},T}(v|u)$，$p_{\mathcal{G},T}(v|u)$就是节点 $(u,v)$ 的相似度。
- 通常来说 $T$ 在 $[2,10]$ 之间。

### 4.3.2 解码器预测相似度
- 在简单版本中，我们用点乘来定义两节点相似：
$${\rm{similarity}}(u,v)\ {\approx}\ {z_v}^{\top}z_u$$
- 在一个随机游走序列中，节点 $u$ 以概率 $p_{\mathcal{G},T}(v|u)$ 和节点 $v$ 相似，因此我们可以对 $u$ 的 $T$ 步邻点集做一个softmax来近似这个概率：
$${\rm{DEC}}(z_u,z_v)\triangleq\frac{e^{z_u^{\top}z_v}}{\sum_{{v_k}\in\mathcal{V}}e^{z_u^{\top}z_k}}\ {\approx}\ p_{\mathcal{G},T}(v|u)$$

### 4.3.3 解码器优化迭代
- 最小化交叉熵损失
$$\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}-{\rm{log}}({\rm{DEC}}(z_u,z_v))$$
- $\mathcal{D}$ 是随机游走的训练集，可以直接在节点 $u$ 通过采样的方式获得。

### 4.3.4 解码器迭代算法优化方案
由于每次需要在 $T$ 步邻域 $\mathcal{D}$ 中进行随机采样，在计算softmax时需要计算邻域内每个节点，算法的复杂度较高，成本昂贵。
- 负采样：正样本就是节点 $v$ ，负样本可以直接在节点集 $\mathcal{V}$ 按照概率分布 $P_n(\mathcal{V})$ 进行采样：
$$\mathcal{L}=\sum\limits_{(u,v)\in\mathcal{D}}-{\rm{log}}(\sigma(z_u^{\top}z_v))-\gamma\mathbb{E}_{v_n{\sim}P_n(\mathcal{V})}[{\rm{log}}(-\sigma(z_u^{\top}z_{v_n}))]$$
其中 $\sigma$ 是logistic function，$\gamma$ 是一个正的超参数。
- 概率分布 $P_n(\mathcal{V})$ 可以是均匀分布，在word2vec中一般会根据词频来进行非均匀分布，因此需要根据节点的其他属性来设计。
- 期望的计算可以采用蒙特卡洛法吉布斯抽样。

## 4.4 优化随机游走的策略-Node2Vec
2阶随机游走，在随机游走的过程中，记录着上一个节点。
- 从节点 $u$ 出发，以BFS策略在 $u$ 的邻域随机游走；以DFS策略沿某路径一直游走。通过以下两个参数进行控制：</br>
1.Return参数 $p$ ：以 $\frac{1}{p}$ 的概率返回到上一个节点</br>
2.In-out参数 $q$ ：以 $\frac{1}{q}$ 的概率走向更远的节点</br>
3.以"1"的概率走向其它节点，再对概率进行归一化，得到最终的概率，采样出下一个随机游走。
- $p$ 大 $q$ 小对应于DFS，探索全局宏观区域；$q$ 大 $p$ 小对应于BFS，探索局部微观区域。

## 4.5 随机游走的缺点
- 对新加入的节点的泛化性较差，某种程度的过拟合
- 会更多采样到地理上相近的节点，难以提取到距离远结构同的节点
- 仅利用了图本身的信息，没有运用节点的属性信息

## 4.6 全图嵌入
### 4.6.1 特征求和
- 对全图所有节点的特征求和，作为全图嵌入：
$$z_G=\sum\limits_{v{\in}G}z_v$$
### 4.6.2 虚拟节点
- 引入一个虚拟节点，虚拟节点和 $G$ 的一个子图中所有节点相连，通过求虚拟节点的图嵌入来表示子图的嵌入；或者将子图看作一个节点进行节点嵌入
### 4.6.3 匿名随机游走
![](./img/anonymous.png ':size=60%')
- 每次随机游走记录编号而不记录节点，走到相同的节点就会记录相同的编号。
- 例如上图中Random Walk1和Random Walk2会得到相同的匿名随机游走。
- 下图展示了：随着节点数的增加，匿名随机游走序列的数量呈指数级增长
![](./img/growth.png ':size=60%')
### 4.6.4 通过匿名随机游走构建全图特征
#### 方案1
- 可以用"采样出不同匿名随机游走序列的个数"构成一个向量作为全图特征。
- 匿名随机游走序列长度固定时，想要使误差大于 $\epsilon$ 的概率小于 $\delta$ ，需要采样 $m$ 次：
$$m=\lceil\frac{2}{\epsilon^2}({\rm{log}}(2^{\eta}-2)-{\rm{log}}(\delta))\rceil$$
$\eta$ 是长度 $l$ 的匿名随机游走总数量。
#### 方案2
- 构建一个自监督学习的任务，每种匿名随机游走序列单独学习嵌入编码，全图也单独学习嵌入编码，得到一个2维向量，用来预测匿名随机游走概率。

## 4.7 本章总结
本章主要学习图的嵌入表示学习，主要包括：
- 图嵌入的基本框架：Encoder-Decoder
- 随机游走方法计算节点相似度
- 一些构建全图嵌入的方法